{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "f = open('data', 'wb')\n",
    "df_test = pd.read_csv('test.csv', sep=',')\n",
    "pat = re.compile(f'[{string.punctuation}]')\n",
    "df_test = df_test.replace(pat, '')\n",
    "\n",
    "df_test['question1'] = df_test['question1'].str.lower()\n",
    "df_test['question1'] = df_test['question1'].str.split()\n",
    "df_test['question2'] = df_test['question2'].str.lower()\n",
    "df_test['question2'] = df_test['question2'].str.split()\n",
    "del pat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sasha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "#лемматизация\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "def stop_words(text):\n",
    "    if type(text) is not list:\n",
    "        return []\n",
    "    return list(set(text) - en_stops)\n",
    "\n",
    "df_test['q1_lemmatized'] = df_test['question1'].apply(stop_words)\n",
    "df_test['q2_lemmatized'] = df_test['question2'].apply(stop_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b409fa33ad33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'averaged_perceptron_tagger'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if type(text) is not list:\n",
    "        return []\n",
    "    tags = list(map(lambda x: tag_dict.get(x[1][0].upper(), wordnet.NOUN), nltk.pos_tag(text)))\n",
    "    return [lemmatizer.lemmatize(w, t) for w, t in zip(text, tags)]\n",
    "\n",
    "\n",
    "df_test['q1_lemmatized'] = df_test['q1_lemmatized'].apply(lemmatize_text)\n",
    "df_test['q2_lemmatized'] = df_test['q2_lemmatized'].apply(lemmatize_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle.dump(df_test, open(\"lem_df\", \"wb\"))\n",
    "df_test = pickle.load(open(\"dumped\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#для каждого слова из 1 вопроса считает количество документов, которые его сожержат\n",
    "import numpy as np\n",
    "\n",
    "def func(x) :\n",
    "    if type(x) is list :\n",
    "        list(set(x)) \n",
    "    else:\n",
    "        list()\n",
    "\n",
    "list1 = np.hstack(np.array(list(map(func, df_test[\"q1_lemmatized\"].tolist()))))\n",
    "q1 = Counter(list1)\n",
    "del list1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#для каждого слова из 2 вопроса считает количество документов, которые его сожержат\n",
    "list2 = np.hstack(np.array(list(map(lambda x: list(set(x)) if type(x) is list else list(), df_test[\"q2_lemmatized\"].tolist()))))\n",
    "q2 = Counter(list2)\n",
    "del list2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "q = q1 + q2\n",
    "del q1\n",
    "del q2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#считаем tf\n",
    "import numpy as np\n",
    "def tf(val):\n",
    "    if type(val) is list:\n",
    "        q_keys =  Counter(val).keys()\n",
    "        q_vals = np.array(list(Counter(val).values()))\n",
    "        q_vals = q_vals / len(val)\n",
    "        return dict(zip(q_keys, q_vals))\n",
    "    else:\n",
    "        return dict()\n",
    "    \n",
    "df_test[\"tf1\"] = df_test[\"q1_lemmatized\"].apply(tf)\n",
    "df_test[\"tf2\"] = df_test[\"q2_lemmatized\"].apply(tf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# считаем idf\n",
    "q_keys = q.keys()\n",
    "q_vals = np.array(list(q.values()))\n",
    "q_vals = np.log(len(df_test) / q_vals)\n",
    "idf = dict(zip(q_keys, q_vals))\n",
    "del q_keys\n",
    "del q_vals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#считаем tf-idf\n",
    "\n",
    "def mul_idf(val):\n",
    "    for word in val:\n",
    "        val[word] *= idf[word]\n",
    "    return val\n",
    "\n",
    "df_test[\"tf-idf1\"] = df_test[\"tf1\"].apply(mul_idf)\n",
    "df_test[\"tf-idf2\"] = df_test[\"tf2\"].apply(mul_idf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df_test[\"words_1\"] = df_test[\"tf-idf1\"].apply(lambda x: set(x.keys()))\n",
    "df_test[\"words_2\"] = df_test[\"tf-idf2\"].apply(lambda x: set(x.keys()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#делаем общий вектор из слов двух предложений\n",
    "def or_func(x):\n",
    "    return list(x[\"words_1\"] | x[\"words_2\"])\n",
    "\n",
    "df_test[\"all_words\"] = df_test.apply(or_func, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\sasha\\pycharmprojects\\quora\\venv\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "c:\\users\\sasha\\pycharmprojects\\quora\\venv\\lib\\site-packages\\scipy\\spatial\\distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "c:\\users\\sasha\\pycharmprojects\\quora\\venv\\lib\\site-packages\\numpy\\lib\\function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "c:\\users\\sasha\\pycharmprojects\\quora\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2345796.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57158f2ebbf04509aa292422242a9c6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#создаем два вектора и считаем косинус между ними\n",
    "from scipy.spatial import distance\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "def cosine(x):\n",
    "    l = len(x[\"all_words\"])\n",
    "    a1 = [0] * l\n",
    "    a2 = [0] * l\n",
    "    for i in range(l):\n",
    "        if x[\"all_words\"][i] in x[\"tf-idf1\"]:\n",
    "            a1[i] = x[\"tf-idf1\"][x[\"all_words\"][i]]\n",
    "        if x[\"all_words\"][i] in x[\"tf-idf2\"]:\n",
    "        \n",
    "            a2[i] = x[\"tf-idf2\"][x[\"all_words\"][i]]\n",
    "    return distance.cosine(a1, a2)\n",
    "\n",
    "df_test[\"cosine\"] = df_test.progress_apply(cosine, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "df_test[\"cosine\"] = df_test[\"cosine\"].fillna(1)\n",
    "df_test[\"cosine\"] = df_test[\"cosine\"].apply(lambda x: 1 - x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#записываем результат в файл simple_xgb.csv\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = df_test['cosine']\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}